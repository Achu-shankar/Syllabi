from typing import List, Tuple, Dict, Any
from uuid import UUID
import logging

from supabase import Client # Changed to sync client
# from supabase_py_async import AsyncClient

from app.schemas.chunk import ChunkCreate, MultimediaChunkCreate # Used for type hinting and accessing chunk data
# We don't directly use the full Chunk schema here as we are creating records.

from app.core.config import settings # Import settings

logger = logging.getLogger(__name__)

# Name of your Supabase table for storing document chunks
DOCUMENT_CHUNKS_TABLE_NAME = "document_chunks"
# DB_INSERT_BATCH_SIZE will be taken from settings

def bulk_create_chunks_with_embeddings(
    db: Client, # Changed to sync client
    chunk_embeddings_data: List[Tuple[ChunkCreate, List[float]]]
) -> Tuple[List[Dict[str, Any]], str | None]:
    """
    Performs a bulk insert of document chunks with their embeddings into the Supabase table,
    handling large numbers of chunks by breaking them into smaller batches.

    Args:
        db: The Supabase sync client instance.
        chunk_embeddings_data: A list of tuples, where each tuple contains:
            - chunk_info (ChunkCreate): The Pydantic model with chunk details.
            - embedding_vector (List[float]): The embedding vector for the chunk.

    Returns:
        A tuple containing:
        - List[Dict[str, Any]]: The list of ALL successfully inserted data records from Supabase.
        - str | None: An error message string if any part of the batch operations failed, otherwise None.
    """
    if not chunk_embeddings_data:
        return [], "No chunk data provided for insertion."

    all_inserted_records: List[Dict[str, Any]] = []
    cumulative_error_messages = []

    records_to_prepare = []
    for chunk_info, embedding_vector in chunk_embeddings_data:
        # Convert constituent_elements (List[ParsedTextElement]) to a list of dicts for JSONB storage
        constituent_elements_json_serializable = [
            element.model_dump() for element in chunk_info.constituent_elements
        ]

        record = {
            "reference_id": str(chunk_info.reference_id), # Ensure UUIDs are strings for Supabase client
            "user_id": str(chunk_info.user_id),  # Convert UUID to string for database
            "chatbot_id": str(chunk_info.chatbot_id),
            "page_number": chunk_info.page_number,
            "chunk_text": chunk_info.chunk_text,
            "token_count": chunk_info.token_count,
            "embedding": embedding_vector, # pgvector expects a list of floats
            "constituent_elements_data": constituent_elements_json_serializable, # Stored as JSONB
            # "parser_metadata": chunk_info.parser_metadata # Already a dict or None
            # chunk_id and created_at will be auto-generated by the database
        }
        records_to_prepare.append(record)

    total_records_to_insert = len(records_to_prepare)
    logger.info(f"Preparing to bulk insert {total_records_to_insert} total chunks in batches of {settings.DB_INSERT_BATCH_SIZE}.")

    for i in range(0, total_records_to_insert, settings.DB_INSERT_BATCH_SIZE):
        batch_to_insert = records_to_prepare[i:i + settings.DB_INSERT_BATCH_SIZE]
        batch_number = (i // settings.DB_INSERT_BATCH_SIZE) + 1
        logger.info(f"Attempting to insert batch {batch_number} ({len(batch_to_insert)} chunks) into '{DOCUMENT_CHUNKS_TABLE_NAME}'.")
        
        try:
            response = db.table(DOCUMENT_CHUNKS_TABLE_NAME).insert(batch_to_insert).execute()

            if response.data:
                logger.info(f"Successfully inserted {len(response.data)} chunk records from batch {batch_number}.")
                all_inserted_records.extend(response.data)
            else:
                error_msg = f"Supabase insert operation for batch {batch_number} returned no data."
                if hasattr(response, 'error') and response.error:
                    error_msg = f"Supabase insert error for batch {batch_number}: {response.error.message if response.error else 'Unknown error'}"
                logger.error(error_msg)
                cumulative_error_messages.append(error_msg)
                # Optionally, decide if one batch failure should stop all subsequent batches
                # For now, we attempt all batches and report cumulative errors.
                
        except Exception as e:
            logger.error(f"Error during bulk insert of chunks (batch {batch_number}): {e}", exc_info=True)
            error_detail = str(e)
            if hasattr(e, 'message') and e.message:
                error_detail = e.message
            cumulative_error_messages.append(f"Failed to insert batch {batch_number}: {error_detail}")
    
    if cumulative_error_messages:
        # If any batch failed, the overall operation is considered to have issues.
        # The caller can check the length of all_inserted_records against total_records_to_insert.
        final_error_message = "One or more batches failed during chunk insertion. Errors: " + "; ".join(cumulative_error_messages)
        logger.error(final_error_message)
        return all_inserted_records, final_error_message # Return records that *were* inserted, plus errors
    
    logger.info(f"Successfully inserted all {len(all_inserted_records)} chunks across all batches.")
    return all_inserted_records, None


# ============================================================================
# MULTIMEDIA CHUNK STORAGE FUNCTIONS
# ============================================================================

def bulk_create_multimedia_chunks_with_embeddings(
    db: Client,
    multimedia_chunk_embeddings_data: List[Tuple[Dict[str, Any], List[float]]],
    user_id: UUID,
    content_type: str
) -> Tuple[List[Dict[str, Any]], str | None]:
    """
    Performs a bulk insert of multimedia chunks with their embeddings into the document_chunks table.
    
    Args:
        db: The Supabase sync client instance.
        multimedia_chunk_embeddings_data: List of tuples containing:
            - chunk_data (Dict): Multimedia chunk data from MultimediaChunkingService
            - embedding_vector (List[float]): The embedding vector for the chunk
        user_id: UUID of the user who owns this content
        content_type: Content type from source (VIDEO/AUDIO)
        
    Returns:
        A tuple containing:
        - List[Dict[str, Any]]: Successfully inserted records from Supabase
        - str | None: Error message if any batch operations failed, otherwise None
    """
    if not multimedia_chunk_embeddings_data:
        return [], "No multimedia chunk data provided for insertion."
    
    all_inserted_records: List[Dict[str, Any]] = []
    cumulative_error_messages = []
    
    # Convert content type to lowercase for database
    db_content_type = content_type.lower() if content_type in ["VIDEO", "AUDIO"] else "audio"
    
    records_to_prepare = []
    for chunk_data, embedding_vector in multimedia_chunk_embeddings_data:
        # Map multimedia chunk data to database schema
        record = {
            "reference_id": str(chunk_data.get("reference_id")),
            "user_id": str(user_id),
            "chatbot_id": str(chunk_data.get("chatbot_id")),
            "page_number": 0,  # Not applicable for multimedia, use 0 as default
            "chunk_text": chunk_data.get("text", ""),
            "token_count": chunk_data.get("word_count", 0),
            "embedding": embedding_vector,
            
            # Multimedia-specific fields (new columns)
            "content_type": db_content_type,  # Use the actual content type from source
            "start_time_seconds": int(chunk_data.get("start_time", 0)),
            "end_time_seconds": int(chunk_data.get("end_time", 0)),
            "speaker": chunk_data.get("speaker"),
            "confidence_score": chunk_data.get("confidence_score"),
            "chunk_type": chunk_data.get("chunk_type", "transcript"),
            
            # Store multimedia metadata as JSONB
            "constituent_elements_data": {
                "multimedia_metadata": {
                    "duration": chunk_data.get("duration", 0),
                    "segment_count": chunk_data.get("segment_count", 0),
                    "word_count_precise": chunk_data.get("word_count_precise", 0),
                    "language": chunk_data.get("language", "auto-detected"),
                    "segments": chunk_data.get("segments", []),
                    "words": chunk_data.get("words", [])
                }
            }
        }
        
        records_to_prepare.append(record)
    
    total_records_to_insert = len(records_to_prepare)
    batch_size = getattr(settings, 'DB_INSERT_BATCH_SIZE', 50)  # Default to 50 if not set
    
    logger.info(f"Preparing to bulk insert {total_records_to_insert} multimedia chunks in batches of {batch_size}.")
    
    for i in range(0, total_records_to_insert, batch_size):
        batch_to_insert = records_to_prepare[i:i + batch_size]
        batch_number = (i // batch_size) + 1
        logger.info(f"Attempting to insert multimedia batch {batch_number} ({len(batch_to_insert)} chunks) into '{DOCUMENT_CHUNKS_TABLE_NAME}'.")
        
        try:
            response = db.table(DOCUMENT_CHUNKS_TABLE_NAME).insert(batch_to_insert).execute()
            
            if response.data:
                logger.info(f"Successfully inserted {len(response.data)} multimedia chunk records from batch {batch_number}.")
                all_inserted_records.extend(response.data)
            else:
                error_msg = f"Supabase insert operation for multimedia batch {batch_number} returned no data."
                if hasattr(response, 'error') and response.error:
                    error_msg = f"Supabase insert error for multimedia batch {batch_number}: {response.error.message if response.error else 'Unknown error'}"
                logger.error(error_msg)
                cumulative_error_messages.append(error_msg)
                
        except Exception as e:
            logger.error(f"Error during bulk insert of multimedia chunks (batch {batch_number}): {e}", exc_info=True)
            error_detail = str(e)
            if hasattr(e, 'message') and e.message:
                error_detail = e.message
            cumulative_error_messages.append(f"Failed to insert multimedia batch {batch_number}: {error_detail}")
    
    if cumulative_error_messages:
        final_error_message = "One or more multimedia batches failed during chunk insertion. Errors: " + "; ".join(cumulative_error_messages)
        logger.error(final_error_message)
        return all_inserted_records, final_error_message
    
    logger.info(f"Successfully inserted all {len(all_inserted_records)} multimedia chunks across all batches.")
    return all_inserted_records, None


def delete_chunks_by_reference_id(
    db: Client, # Changed to sync client
    reference_id: UUID
) -> Tuple[bool, str | None]:
    """
    Deletes all document chunks associated with a given reference_id.

    Args:
        db: The Supabase sync client instance.
        reference_id: The UUID of the reference whose chunks are to be deleted.

    Returns:
        A tuple containing:
        - bool: True if deletion was successful or no matching rows were found, False on error.
        - str | None: An error message string if an error occurred, otherwise None.
    """
    try:
        logger.info(f"Attempting to delete chunks for reference_id: {reference_id} from '{DOCUMENT_CHUNKS_TABLE_NAME}'.")
        response = db.table(DOCUMENT_CHUNKS_TABLE_NAME).delete().eq("reference_id", str(reference_id)).execute()

        if hasattr(response, 'error') and response.error:
            error_msg = f"Supabase delete error: {response.error.message if response.error else 'Unknown error'}"
            logger.error(error_msg)
            return False, error_msg
        else:
            # Log how many were (potentially) deleted. Response.data might be empty if nothing matched.
            count_deleted = len(response.data) if response.data else 0
            logger.info(f"Delete operation for reference_id '{reference_id}' completed. Matched and potentially deleted {count_deleted} chunk(s).")
            return True, None # Success, even if 0 rows were deleted because they didn't exist.
            
    except Exception as e:
        logger.error(f"Error deleting chunks for reference_id '{reference_id}': {e}", exc_info=True)
        error_detail = str(e)
        if hasattr(e, 'message') and e.message:
            error_detail = e.message
        return False, f"Failed to delete chunks: {error_detail}"

# Example Usage (illustrative, not for direct execution without async setup or Supabase client):
# def main_example_sync():
#     from app.core.supabase_client import get_supabase_client # Assuming this returns a sync Client
#     supabase_client = get_supabase_client()
#     
#     if not supabase_client:
#         logger.error("Failed to get Supabase client for example.")
#         return

#     # Dummy data for testing
#     sample_ref_id = UUID("a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11")
#     sample_user_id = "test_user_crud"
#     sample_project_id = UUID("b1eebc99-9c0b-4ef8-bb6d-6bb9bd380a22")
#     sample_embedding = [0.1] * 1536 # Correct dimension

#     sample_element = ParsedTextElement(
#         text="hello", x0=1, y0=1, x1=2, y1=2, page_number=1, page_width=100, page_height=100
#     )
#     chunk_to_create_1 = ChunkCreate(
#         reference_id=sample_ref_id, user_id=sample_user_id, project_id=sample_project_id,
#         page_number=1, chunk_text="Hello world for CRUD", token_count=4,
#         constituent_elements=[sample_element]
#     )
#     chunk_to_create_2 = ChunkCreate(
#         reference_id=sample_ref_id, user_id=sample_user_id, project_id=sample_project_id,
#         page_number=2, chunk_text="Second chunk here", token_count=3,
#         constituent_elements=[sample_element]
#     )

#     data_to_insert = [
#         (chunk_to_create_1, sample_embedding),
#         (chunk_to_create_2, sample_embedding) # Using same embedding for simplicity
#     ]

#     # Test deletion first (clean slate for the reference_id)
#     logger.info(f"Attempting to delete existing chunks for reference ID: {sample_ref_id}")
#     deleted_ok, delete_error = delete_chunks_by_reference_id(supabase_client, sample_ref_id)
#     if delete_error:
#         logger.error(f"Pre-test deletion failed: {delete_error}")
#     else:
#         logger.info(f"Pre-test deletion status for reference ID {sample_ref_id}: {deleted_ok}")

#     # Test bulk creation
#     logger.info("Attempting to bulk create chunks...")
#     inserted_data, insert_error = bulk_create_chunks_with_embeddings(supabase_client, data_to_insert)
#     if insert_error:
#         logger.error(f"Bulk create failed: {insert_error}")
#     else:
#         logger.info(f"Bulk create successful. Inserted {len(inserted_data)} records.")
#         # for record in inserted_data:
#         #     logger.info(f"  Inserted chunk_id: {record.get('chunk_id')}")
#     
#     # Optional: Test deletion again to clean up
#     # await delete_chunks_by_reference_id(supabase_client, sample_ref_id)

# if __name__ == "__main__":
#     logging.basicConfig(level=logging.INFO)
#     # main_example_sync()
#     logger.info("CRUD example finished. Adapt main_example_sync and ensure sync Supabase client setup to run.") 